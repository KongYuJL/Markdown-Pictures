## 视觉里程计 1

第七章主要讲的是SLAM中的前端：**视觉里程计（visual odometry）**, 简称VO。
VO非常重要，与同时定位与建图中的其一，定位，密切相关。VO的输入可以是单张RGB图片，也可以是RGBD，还可以包含IMU数据，甚至可以是前后两张图片同时输入（也许也能是视频流）。

所谓IMU，就是惯性测量单元，比如陀螺仪等等，计算定位对象的加速度之类的信息。最后输出的是定位对象的位姿信息，就是旋转向量和平移向量。

不过VO预测的是相对运动，在绝对定位场景中，会设第一帧图像的位置为参考坐标系，或者以第一帧图像的相对位置作为参考坐标系。

如何将图片之类的输入数据，处理成我们想要的输出，位姿信息？这里面就有很多实现方法，主要分为两种，一种是特征点法，一种是不提取特征的直接法。目前的主流是基于特征点法，更有效、鲁棒。

本章主要讲了以下内容：

1. 特征点法：找到两张2D图像上的匹配点。
2. 对极几何：根据2D-2D特征点对求解R,t。
3. 三角测量：根据2D-2D特征点求深度。
4. PnP：根据3D点云和匹配的2D图像求R,t。
5. ICP：求两个点云之间的R,t。
其关系可参考下图，来自[博客](https://blog.csdn.net/qq_23225073/article/details/78452638#31-%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5essential-matrixe)：
![参考图片](https://img-blog.csdn.net/20171113122733673?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjMyMjUwNzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 1.特征点法

什么是特征点呢？首先，计算机中的图像，本质是一个由亮度和色彩组成的多维矩阵。如果直接从矩阵层次考虑运动估计，效率会很低，会引入许多噪声。因此传统的解决方案，是先从图像中选取比较**有代表性**的点。这些点通常具有**可重复性、可区别性、高效率、本地性**。在经典SLAM中，也称为**路标**，在视觉SLAM中，则指**图像特征**(Feature)。

特征点通常由关键点（key-point）和描述子(descriptor)两部分组成。先提取“关键点”，再计算“描述子”。常见的特征点有**SIFT**(尺度不变特征变换, Scale-Invariant Feature Transform)，**SURF**(Speeded up Robust Features), **ORB**(Oriented Fast and Rotated BRIEF)，其中ORB是最具有代表性的实时图像特征（SIFT太慢，FAST没有方向信息）。

#### 1.1 ORB特征

提取ORB特征的两个步骤：

+ **Oriented FAST角点（关键点）提取**

+ **BRIEF描述子**

**关键点：Oriented FAST**

ORB特征使用的FAST关键点是改进的Oriented FAST。FAST主要检测局部像素灰度变化明显的地方，以速度快著称。它的核心思想是：如果一个像素与邻域像素的差别较大（过亮或过暗），那么它更可能是角点（关键点）。具体的流程见书。

![FAST特征点](https://img-blog.csdn.net/20170220092211672?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzYwMjI3MzA5MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

原始的FAST角点经常出现“扎堆”的现象，还需要使用NMS(极大值抑制，non-maximal suppresion)，避免角点集中问题。ORB还对原始FAST角点计算**Harris响应值**，然后选取前N个具有最大响应值的角点。此外，改进的FAST关键点添加了方向和尺度信息。尺度不变性由构建图像金字塔（多次不同层次的降采样），并在金字塔的每一层上检测关键点来实现。而特征的旋转由灰度质心法来表示，步骤如下：

(1) 在一个小的图像块B中，定义图像块的矩

$$
m_{p q}=\sum x^{p} y^{q} I(x, y), \ \ \ p, q= \{0,1\}
$$

(2) 计算图像块的质心
$$
C=\left(\frac{m_{10}}{m_{00}}, \frac{m_{01}}{m_{00}}\right)=\left(\frac{\sum x I(x, y)}{\sum I(x, y)}, \frac{\sum y I(x, y)}{\sum I(x, y)}\right)
$$
(3) 计算特征点方向，连接图像块的几何中心O和质心C，可以得到方向向量 $\vec{OC}$，因此特征点的方向可以定义为：
$$
\theta=\arctan \left(\frac{m_{01}}{m_{10}}\right)=\arctan \left(\frac{\sum y I(x, y)}{\sum x I(x, y)}\right)
$$
这样得到的Oriented FAST关键点，具有更好的鲁棒性。

**描述子：BRIEF**

**BRIEF**(Binary Robust Independent Elementary Feature)，是一种二进制描述子，描述向量是由许多个0和1组成，这里的0和1编码了**关键点附近的两个像素**$p$和$q$的大小关系，如果$p>q$,则取1；反之取0。具体像素的挑选是按照某个特定的分布进行随机，具体可以查看OpenCV代码或者原论文。

BRIEF & rBRIEF [Ref](https://raw.githubusercontent.com/ez4lionky/Markdown-Pictures/master/2019-12-12Visual-SLAM/img2.png):

![BRIEF](https://raw.githubusercontent.com/ez4lionky/Markdown-Pictures/master/2019-12-12Visual-SLAM/img2.png)

![rBRIEF](https://raw.githubusercontent.com/ez4lionky/Markdown-Pictures/master/2019-12-12Visual-SLAM/img3.png)

#### 1.2 特征匹配

特征点是具有代表性的点（相当于人现实世界定位时使用的参照物），而特征匹配解决了SLAM中的数据关联问题(data association)，即确定当前帧的图像特征与前一帧的图像对应关系，通过描述子的差异判断哪些特征为同一个点。再使用相同特征点的相对位置关系，就可以计算出RT（对极几何）。

在图像$I_t$中提取到特征点$x_t^m,m=1,2,...M$，在图像$I_{t+1}$中提取到特征点$x_{t+1}^n,n=1,2,...N$，寻找这两个集合之间的对应关系就是特征匹配。最简单的方式是使用暴力匹配方法(Brute-Force Matcher)。即对两个集合之间的所有点计算描述子的距离，然后排序，取最近的点。**描述子距离**表示两个特征之间的相似程度，在实际应用中取不同的距离度量范数。

+ 浮点类型的描述子使用欧式距离度量
+ 二进制类型描述子(eg: BRIEF)使用汉明距离(Hamming Distance)作为度量

特征点数量过多时，暴力匹配算法会非常慢，不满足SLAM的实时性要求，因此OpenCV中使用的是**快速最近邻方法（FLANN）**。但由于图像特征的局部特性，误匹配的情况广泛存在，比如场景中经常存在大量的重复纹理，使得特征非常相似。因此，这种情况下仅利用局部特征解决误匹配是非常困难的。

### 2. 2D-2D：对极几何

#### 2.1 对极约束

假设从两张图像中得到了一对成1功匹配的特征点，如下图所示。
![对极几何约束](https://raw.githubusercontent.com/ez4lionky/Markdown-Pictures/master/2019-12-12Visual-SLAM/img1.jpg)

其中$I_1$和$I_2$为对应两帧图像，设相机运动在两帧间的运动为$R, t$，两个相机中心分别为$O_1, O_2$。$p_1, p_2$分别为两帧间对应的特征点。二者是通过正确的匹配得到的，说明这两个特征点是**同一个空间点在两个成像平面上的投影**。并且连线$\vec{O_1p_1}$和连线$\vec{O_1p_2}$在三维空间中会相交于点P，三点可以确定一个平面，该平面就是**极平面(Epipolar plane)**。$O_1O_2$被称为**基线**(Baseline)，基线与两个像平面的交点$e_1, e_2$为**极点**(Epipoles)，极平面与两个像平面$I_1, I_2$之间的相交线$l_1, l_2$为**极线**(Epipolar line)。

如果没有正确匹配的一对特征点，从$p1$中我们只能推断出空间点在$O_1p_1$射线上的任意位置，而在$p2$上的投影就可能是极线$e_2p_2$上的任一像素点。而在有正确匹配特征点对的情况下，我们就可以推断出空间点P的位置，甚至恢复出相机运动。

再从代数的角度来表示图中的几何关系：

假设在第一帧的坐标系下，P的空间位置为$P=[X, Y, Z]^{T}$。

根据针孔模型，像素点$p_1, p_2$在像素平面的位置为：
$$
s_1p_1 = KP, s_2p_2 = K(RP+t) \\
p_2=K(RK^{-1}p_1 + t)
$$
等号两边同时左乘$K^{-1}$，得到：
$$
K^{-1}p_2=RK^{-1}p_1 + t
$$
设$x_1=K^{-1}p_1,x_2=K^{-1}p_2$并代入，有：
$$
x_2=Rx_1+t
$$
两式同时左乘$t$的反对称矩阵$t^{\wedge}$（$t^{\wedge}t=t \times t$）:
$$
x_2^Tt^{\wedge}{\times}x_2=x_2^Tt^{\wedge}Rx_1 \\
$$
以及$x_2^T$：
$$
x_2^Tt^{\wedge}Rx_1=0
$$
重新换回$p_1, p_2$，得：
$$
p_2^TK^{-T}t^{\wedge}RK^{-1}p_1=0
$$
以上两个式子都是**对极约束**，最后一个式子中只包含像点，相机的旋转和平移，中间的矩阵就是**基础矩阵**$F$(Fundamental Matrix)和**本质矩阵E**(Essential matrix)，
$$
E = t^{\wedge}R, \  F=K^{-T}EK^{-1}, \  p_2^TFp_1 = 0
$$
本质矩阵和基础矩阵相差相机的内参$K$，因此在计算位姿时，通常使用形式更简单的本质矩阵：

1. 先根据配对点的像素位置求出E
2. 根据E求出R, t

因为本质矩阵$E=t^{\wedge}R$是$3 \times 3$的矩阵，共含有九个未知数，利用尺度不变性可以减去一个自由度，即可使用八点法进行求解。求出本质矩阵后再使用矩阵分解，即可得到相机位姿R，t（八点法和矩阵分解详见书）。

对极几何一般是用于单目SLAM的初始化，用对极几何可求出初始位姿，再使用三角测量估计出三维空间点的位置后，就可以使用其他更准确的方法（如PnP）继续求解了。

如果有若干对这样的匹配点，就可以通过这些二维图像特征点的对应关系，恢复出相机的运动。

#### 2.2 单应矩阵

除了基本矩阵和本质矩阵之外，还有**单应矩阵**（Homography）$H$，其描述了两个平面之间的映射关系。

**单应(Homography)**是射影几何中的概念，又称为射影变换。它把一个射影平面上的点(三维齐次矢量)映射到另一个射影平面上，并且把直线映射为直线，具有保线性质。总的来说，单应是关于三维齐次矢量的一种线性变换，可以用一个$3\times3$的非奇异矩阵$H$表示。

若场景中的特征点都落在一个平面上，(比如墙、地面，使用无人机)，则可以通过单应性进行运动估计。

考虑在图像$I_1, I_2$有一对匹配好的特征点$p_1, p_2$。假设这些特征点落在平面$P$上，设这个平面满足方程：



***注：本文中书指代《视觉SLAM十四讲》***