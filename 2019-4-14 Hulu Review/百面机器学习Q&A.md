#### 1. 为什么需要对数值类型的特征做归一化? (1)
**常见Methods:**
Min-max Scaling:

对原始数据进行线性变换，映射到[0, 1]的范围。
$$
X_{norm} = \frac{X - X_{min}} {X_{max}-X_{min}}
$$
Z-Score Normalization（标准化）:
$$
z = \frac {x- \mu} {\sigma}
$$
将原始数据映射到均值为0、标准差为1的分布上，

因为 $E(x) - \mu = 0 $, $SD(\frac{x}{\sigma}) = 1 $ 。

**答：**在进行梯度下降时,学习率一致,但各特征尺度不一致,导致需要更多的迭代才能找到最优解
**补充:**

- 可以提升模型精度,如knn(或者进行PCA降维)计算样本之间的距离,如果一个特征值域范围非常大,那么距离计算就主要依赖这个特征(对特征进行无量纲化)

- 可以防止梯度爆炸与数值溢出

- 归一化是为了消除量纲差异，标准化不改变原始数据的分布，影响体现在几何分布上

- 什么时候用归一化？什么时候用标准化？

  （1）如果对输出结果范围有要求，用归一化。

  （2）如果数据较为稳定，不存在极端的最大最小值，用归一化。

  （3）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。

#### 2. 在对数据进行预处理时,应该怎样处理类别型特征? (2)

**常见Methods:**

Ordinal encoding;

通常用于处理类别间具有大小关系的数据，如卡方分箱、成绩可以分为高中低（根据大小关系对类别型特征赋予一个数值ID）

One-hot encoding;

通常处理的数据不具有大小关系。缺点是维度过高，可以使用稀疏向量来节省空间，并且使用特征选择来降低维度。

Binary encoding；

主要分为两步，先进行序号编码，再将序号转换为二进制编码，其维数少于独热编码。

...

#### 3.什么是组合特征？如何处理高维组合特征？ 

为了提高复杂关系的拟合能力，特征工程中常常会把一阶离散特征两两组合，构成高阶组合特征。比如推荐系统中用户ID（m个不同取值）和商品ID（n个不同取值）组合，那么组合特征就有m×n个不同取值。但当m和n过大时，组合特征的维度就会变得很大，需要学习的参数规模也为m×n，此时可以利用矩阵分解对特征进行低维表示。

#### 4.怎样有效地找到组合特征？

利用决策树的特征组合选择组合特征：

对于原始数据，可以使用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征（加入原有特征一起或者不加）用来训练模型（LR等等）。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。

#### 5.有哪些文本表示模型？它们各有什么优缺点？

词袋模型与N-gram模型；

**词袋模型**是最基础的文本表示模型。顾名思义，就是把每一篇文章看成一袋子单词，并忽略每个此出现的顺序。具体就是将整段文本以词为单位切分，每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重代表这个词在文章中的重要程度。一般用TF-IDF计算权重,公式如下：
$$
TF-IDF(t, d)=TF(t, d) \times IDF(t)
$$
其中，TF(t, d)为单词t在文档d中出现的频率，IDF(t)是逆文档频率，用来衡量单词t对表达语义所起的重要性，表示为：
$$
IDF(t) =log \frac{N}{N' + 1}
$$
其中，N为文章总数，N'为包含单词t的文章总数+1。直观的解释是，如果一个单词在很多文章中出现，那么它有可能是一个比较通用的单词，对于区分某篇文章的特殊语义的贡献较小，因此对权重作一定的惩罚。缺点：

**N-gram**模型，用词袋模型将文章进行单词级别的划分有的时候未必是一种好的做法，例如：natural language processing，如果将natural, language, processing这三个词拆开，所表达的意思与三个词连在一起时大相径庭。通常，可以将n个连续出现的单词(n<=N)组成的词组(N-gram)也作为一个单独的特征放到向量表示中去，构成N-gram模型。另外，同一个词可能有多种词性变化，但是却有相似的含义。在实际应用中，，一般会对单词进行词干抽取(Word Stemming)处理，即将不同词性的单词统一称为同一词干的形式。

**主题模型：**...

**词嵌入与深度模型：**如Word2Vec将每个词映射成低维空间上的一个稠密向量。

#### 6.Word2Vec是如何工作的？